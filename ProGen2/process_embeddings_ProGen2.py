# This script was used to generate the ESM2 embeddings for the proteins generated by the ProGen2 models.

import torch
import numpy as np
from tqdm import tqdm
import esm
import sys
import time
from pathlib import Path

def load_model_and_alphabet_local(model_location): 
    """Load from local path. The regression weights need to be co-located""" 
    model_location = Path(model_location) 
    model_data = torch.load(str(model_location), map_location="cpu")
    model_name = model_location.stem
    regression_data = None
    return esm.pretrained.load_model_and_alphabet_core(model_name, model_data, regression_data) 

np.random.seed(42)
torch.manual_seed(42)

total_time_start = time.time()

data_path = str(sys.argv[1])
lengths_data_path = str(sys.argv[2])
output_path = str(sys.argv[3])
weights_path = str(sys.argv[4])
start_range = int(sys.argv[5])
end_range = int(sys.argv[6])
esm_transformer_layers = int(sys.argv[7])
esm_model_name = str(sys.argv[8])

print("System arguments read succesfully \n")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Current device:", device.type)

# Define the output path and file name for the embeddings
avg_embeddings_path = output_path + f"ProGen2_x_{esm_model_name}_{start_range}_{end_range}_emb.t"
print(avg_embeddings_path)

batch_size = 16
print("Batch size:", batch_size)

start_time_to_load_model_and_data = time.time()

test = torch.load(data_path)
print("Loaded sequences \n")
test = np.array(test)
test = torch.from_numpy(test)
test = test.to(torch.int64)
print(test.shape)

# Select the test sequences within the range
test_selection = test[start_range:end_range]
print("Taking a selection of the entire data: starting from", start_range, "until", end_range, "length is", len(test_selection), "\n")

lengths = torch.load(lengths_data_path)
lengths = torch.tensor(lengths)
print(lengths.shape)
print("Correct lengths loaded, required to calculate correct average embedding. \n")

lengths = lengths[start_range:end_range]
print(lengths.shape)

dataloader = torch.utils.data.DataLoader(test_selection, batch_size=batch_size, pin_memory=True, shuffle=False)
print("dataloader ok \n")


# Load the ESM-2 model
model, alphabet = load_model_and_alphabet_local(weights_path)
print("model loaded")
model.to(device)
print("model on device", device.type, "\n")
model.eval()

stop_time_to_load_model_and_data = time.time()

# Calculate and save the embeddings for the selected test sequences
representations = []

sequence_representations = []
counter = 0
times_embeddings = []
for i, batch in enumerate(tqdm(dataloader)):
    if i % 250 == 0:
        print(f"Step{i}")

    with torch.no_grad():
        start = time.time()
        results = model(batch.to(device), repr_layers=[esm_transformer_layers], return_contacts=False)
    token_representations = torch.tensor(results["representations"][esm_transformer_layers].cpu())
    for i, tokens_len in enumerate(lengths[counter:counter+batch_size]):
        sequence_representations.append(token_representations[i, 0 : tokens_len ].mean(0))
    counter += batch_size
    stop = time.time()
    times_embeddings.append(stop-start)
sequence_representations = torch.stack(sequence_representations, dim=0)

torch.save(sequence_representations, avg_embeddings_path)
total_time_stop = time.time()

print(f"Saved embeddings for sequences {start_range} to {end_range}")
print("Took ", total_time_stop-total_time_start, "seconds.")

print("sequence_representations.shape", sequence_representations.shape)
print("Time required to load model and data", stop_time_to_load_model_and_data-start_time_to_load_model_and_data)
print("Time required to calculate embeddings WITHOUT the time required to preload the model", 
      sum(times_embeddings), len(times_embeddings))
print("Average time per batch of", batch_size, "proteins through", esm_model_name, sum(times_embeddings)/len(times_embeddings)) 
print("Average time per sequence: ", sum(times_embeddings)/len(times_embeddings)/batch_size)
print("Standard deviation per sequence: ", np.std(times_embeddings)/batch_size)
